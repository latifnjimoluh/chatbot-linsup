# === Auth LLM ===
GEMINI_API_KEY=PASTE_YOUR_KEY_HERE

# === HTTP server ===
PORT=3002
ALLOWED_ORIGIN=http://localhost:5173
LOG_LEVEL=info

# === Modèles ===
MODEL_ID=gemini-1.5-flash
MODEL_FALLBACK_ID=gemini-1.5-flash-8b   # utilisé si quota/erreur sur le modèle principal

# === Timeouts / streaming ===
REQUEST_TIMEOUT_MS=45000
STREAM_SPLIT=char              # char | word | chunk  (granularité SSE côté serveur)
STREAM_DELAY_MS=0              # délai (ms) entre unités envoyées si char/word
STREAM_HEARTBEAT_MS=15000      # ping keep-alive SSE (désactiver: mettre 0)

# === Knowledge Base / Vector index ===
KB_DIR=knowledge_base
VECTOR_DIR=vectorstore
RAG_TOP_K=3                    # nb d'extraits injectés dans le prompt
RAG_MIN_SCORE=0.35             # 0..1 ; mettre "" pour désactiver le filtre
RAG_QVARIANTS=3                # 1..3 (question originale + 2 variantes locales)

# === Cache embeddings (LRU) ===
EMBED_CACHE_SIZE=200
EMBED_CACHE_TTL_MS=1800000     # 30 minutes

# Sélection du backend de recherche vectorielle
SEARCH_BACKEND=json          # json | pinecone | chroma (plus tard)

# Pinecone (si tu l’actives plus tard)
PINECONE_API_KEY=...
PINECONE_INDEX=kb            # nom de l'index
PINECONE_NAMESPACE=default   # optionnel
